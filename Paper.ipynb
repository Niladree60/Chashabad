{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnf0l9ST6qG1mJqzK2nUfS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SX_KqT7QwJVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3158e38-8ed6-4870-cac8-77ca1f283d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ekphrasis in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (5.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.64.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (2.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.7)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (6.1.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (0.4.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.21.6)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->ekphrasis) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "# Import Section\n",
        "import csv\n",
        "import codecs\n",
        "import sys\n",
        "import io\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "# Import Section\n",
        "import csv, codecs, sys, io, re, random, string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer(preserve_case=True, strip_handles=True, reduce_len=True)\n",
        "\n",
        "# For Classifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import model_selection\n",
        "\n",
        "# Python script for confusion matrix creation. \n",
        "from sklearn.metrics import *\n",
        "\n",
        "#Global Initialization Section\n",
        "from ekphrasis.classes.segmenter import Segmenter\n",
        "seg = Segmenter(corpus=\"twitter\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import matplotlib as plt\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import re,nltk,json, pickle\n",
        "import tensorflow as tf\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "from bs4 import BeautifulSoup\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import LSTM,GRU,Bidirectional\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from re import sub\n",
        "import multiprocessing\n",
        "import os\n",
        "from time import time \n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM,Dense,Dropout,Activation,Embedding,Flatten,Bidirectional,MaxPooling1D, Conv1D, MaxPooling1D\n",
        "from keras import regularizers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "##from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import h5py\n",
        "import csv\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers.convolutional import Conv1D \n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "##from keras.preprocessing.sequence import pad_sequences\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "!pip install ekphrasis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib-venn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oqIV4tKaLkH",
        "outputId": "6cbfaa8a-cf98-45f1-a7a8-e4d9f902e964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.7/dist-packages (0.11.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from matplotlib-venn) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from matplotlib-venn) (1.7.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from matplotlib-venn) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->matplotlib-venn) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->matplotlib-venn) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->matplotlib-venn) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->matplotlib-venn) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->matplotlib-venn) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->matplotlib-venn) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For HashtagSegmentation\n",
        "!pip install ekphrasis\n",
        "from ekphrasis.classes.segmenter import Segmenter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUunVMBSwQLT",
        "outputId": "b0d5e3bf-3eee-4e57-ef6c-377e9226650a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ekphrasis in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (6.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.64.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (2.0.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (0.4.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.7)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (5.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.21.6)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->ekphrasis) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Classifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "LAHIDTMMwTz4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa258db8-3dc1-48ab-dedf-ae5954d176f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ3EItCRwdhg",
        "outputId": "9c9e7c2d-af6c-4b6c-edd3-df085f9a873a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python script for confusion matrix creation. \n",
        "from sklearn.metrics import *\n",
        "\n",
        "#Global Initialization Section\n",
        "seg = Segmenter(corpus=\"twitter\")\n",
        "\n",
        "\n",
        "def hashtagSegmentation(text):\n",
        "  givenText=text\n",
        "  getHashTagFromText = [t for t in givenText.split() if t.startswith('#')]\n",
        "  segment_the_Hash =''\n",
        "  getHashtagSegmentedText = givenText\n",
        "  if getHashTagFromText:\n",
        "      for eachHashTag in getHashTagFromText:\n",
        "           eachHashTag=eachHashTag[1:]\n",
        "           segment_the_Hash = seg.segment(eachHashTag)\n",
        "           getHashtagSegmentedText = getHashtagSegmentedText+\" \"+ segment_the_Hash\n",
        "\n",
        "  return getHashtagSegmentedText\n",
        "\n",
        "\n",
        "def preprocess_tweet_text(tweet):\n",
        "    #Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    #Tokenizer\n",
        "    tweet_tokens = tweet_tokenizer.tokenize(tweet)\n",
        "    #tweet_tokens = tweet.split()\n",
        "    filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
        "    from nltk.stem.snowball import SnowballStemmer \n",
        "    #the stemmer requires a language parameter \n",
        "    ss= SnowballStemmer(language='english')\n",
        "    stemmed_words = [ss.stem(w) for w in filtered_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]\n",
        "    \n",
        "    return \" \".join(lemma_words)\n",
        "\n",
        "def preProcessingModule(text):\n",
        "  returnPreProcessedText=\"\"  \n",
        "  getHashtagSegmentedText = hashtagSegmentation(text)\n",
        "  lexicallyNormalizedText = preprocess_tweet_text(getHashtagSegmentedText)\n",
        "\n",
        "\n",
        "  returnPreProcessedText = getHashtagSegmentedText\n",
        "\n",
        "\n",
        "  return returnPreProcessedText\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O677zjCfwj4T",
        "outputId": "df019e82-998f-4150-a781-d0ee7a4f65c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = []\n",
        "label = []\n",
        "csv.field_size_limit(500 * 1024 * 1024)  \n",
        "with open('/content/drive/MyDrive/Dataset/Final_Training.txt', 'r') as f:\n",
        "  next(f) # skip headings\n",
        "  reader=csv.reader(f, dialect=\"excel-tab\")\n",
        "  for line in reader:\n",
        "    #print(line[2])\n",
        "    preProcessedTweetText= preProcessingModule(line[2])\n",
        "    #print(preProcessedTweetText)\n",
        "    tweets.append(preProcessedTweetText)\n",
        "    if(line[1]==\"1\"):\n",
        "      label.append(\"Fake\")\n",
        "    else:\n",
        "      label.append(\"Not Fake\")\n",
        "  #print(label)\n",
        "\n",
        "X_train = np.array(tweets)\n",
        "Y_train = np.array(label)"
      ],
      "metadata": {
        "id": "eU75JJ4Zwo35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########For Decision Tree Classifier:########\n",
        "dt_classifier = Pipeline([\n",
        "     ('count_vectorizer', CountVectorizer(ngram_range=(1, 3))),\n",
        "     ('clf', OneVsRestClassifier(DecisionTreeClassifier(random_state=0)))])\n",
        "  \n",
        "  \n",
        "  ## Train Classifier\n",
        "dt_classifier.fit(X_train, Y_train)\n",
        "  \n",
        "  \n",
        "  ## Test Set Prediction Module\n",
        "testTweets = []\n",
        "testLabelGold = []\n",
        "csv.field_size_limit(500 * 1024 * 1024)  \n",
        "with open('/content/drive/MyDrive/Dataset/Final_Testing.txt', 'r') as f:\n",
        "  next(f) # skip headings\n",
        "  reader=csv.reader(f, dialect=\"excel-tab\")\n",
        "  for line in reader:\n",
        "    #print(line[2])\n",
        "    preProcessedTweetText= preProcessingModule(line[2])\n",
        "    #print(preProcessedTweetText)\n",
        "    testTweets.append(preProcessedTweetText)\n",
        "    if(line[1]==\"1\"):\n",
        "      testLabelGold.append(\"Fake\")\n",
        "    else:\n",
        "      testLabelGold.append(\"Not Fake\")\n",
        "  \n",
        "X_test = np.array(testTweets)\n",
        "testLabelPredicted = dt_classifier.predict(X_test)\n",
        "#print(testLabelPredicted)\n",
        "\n",
        "# Evaluation\n",
        "results = confusion_matrix(testLabelGold, testLabelPredicted) \n",
        "    \n",
        "print ('Confusion Matrix :')\n",
        "print (results) \n",
        "\n",
        "print ('Recall Score :',recall_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Precision Score :',precision_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Macro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Weighted Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='weighted'))\n",
        "  \n",
        "print ('Accuracy :',accuracy_score(testLabelGold, testLabelPredicted)) \n",
        "  \n",
        "  \n",
        "print ('Evaluation Report : ')\n",
        "print (classification_report(testLabelGold, testLabelPredicted)) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5ym9BXrwsnA",
        "outputId": "4afde3d2-a481-4abe-f029-fc884cac5b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix :\n",
            "[[63 37]\n",
            " [39 61]]\n",
            "Recall Score : 0.62\n",
            "Precision Score : 0.6200480192076832\n",
            "Micro Avg. F1 Score : 0.62\n",
            "Macro Avg. F1 Score : 0.61996199619962\n",
            "Micro Avg. F1 Score : 0.62\n",
            "Weighted Avg. F1 Score : 0.61996199619962\n",
            "Accuracy : 0.62\n",
            "Evaluation Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.62      0.63      0.62       100\n",
            "    Not Fake       0.62      0.61      0.62       100\n",
            "\n",
            "    accuracy                           0.62       200\n",
            "   macro avg       0.62      0.62      0.62       200\n",
            "weighted avg       0.62      0.62      0.62       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########For SGD Classifier:########\n",
        "sgd_classifier = Pipeline([\n",
        "     ('count_vectorizer', CountVectorizer(ngram_range=(1, 3))),\n",
        "      ('tfidf', TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)),\n",
        "     ('clf', OneVsRestClassifier(MultinomialNB()))])\n",
        "\n",
        "  \n",
        "  ## Train Classifier\n",
        "sgd_classifier.fit(X_train, Y_train)\n",
        "  \n",
        "  \n",
        "## Test Set Prediction Module\n",
        "testTweets = []\n",
        "testLabelGold = []\n",
        "csv.field_size_limit(500 * 1024 * 1024)  \n",
        "with open('/content/drive/MyDrive/Dataset/Final_Testing.txt', 'r') as f:\n",
        "  next(f) # skip headings\n",
        "  reader=csv.reader(f, dialect=\"excel-tab\")\n",
        "  for line in reader:\n",
        "    #print(line[2])\n",
        "    preProcessedTweetText= preProcessingModule(line[2])\n",
        "    #print(preProcessedTweetText)\n",
        "    testTweets.append(preProcessedTweetText)\n",
        "    if(line[1]==\"1\"):\n",
        "      testLabelGold.append(\"Fake\")\n",
        "    else:\n",
        "      testLabelGold.append(\"Not Fake\")\n",
        "  \n",
        "X_test = np.array(testTweets)\n",
        "testLabelPredicted = sgd_classifier.predict(X_test)\n",
        "#print(testLabelPredicted)\n",
        "\n",
        "# Evaluation\n",
        "results = confusion_matrix(testLabelGold, testLabelPredicted) \n",
        "    \n",
        "print ('Confusion Matrix :')\n",
        "print (results) \n",
        "\n",
        "print ('Recall Score :',recall_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Precision Score :',precision_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Macro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Weighted Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='weighted'))\n",
        "  \n",
        "print ('Accuracy :',accuracy_score(testLabelGold, testLabelPredicted)) \n",
        "  \n",
        "  \n",
        "print ('Evaluation Report : ')\n",
        "print (classification_report(testLabelGold, testLabelPredicted)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT4GPentxHZR",
        "outputId": "55430c0a-13f3-45a4-fa7d-451d33cfe493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix :\n",
            "[[81 19]\n",
            " [54 46]]\n",
            "Recall Score : 0.635\n",
            "Precision Score : 0.6538461538461539\n",
            "Micro Avg. F1 Score : 0.635\n",
            "Macro Avg. F1 Score : 0.6234687298517085\n",
            "Micro Avg. F1 Score : 0.635\n",
            "Weighted Avg. F1 Score : 0.6234687298517084\n",
            "Accuracy : 0.635\n",
            "Evaluation Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.60      0.81      0.69       100\n",
            "    Not Fake       0.71      0.46      0.56       100\n",
            "\n",
            "    accuracy                           0.64       200\n",
            "   macro avg       0.65      0.64      0.62       200\n",
            "weighted avg       0.65      0.64      0.62       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "########For Random Forest Classifier:########\n",
        "rfc_classifier = Pipeline([\n",
        "     ('count_vectorizer', CountVectorizer(ngram_range=(1, 3))),\n",
        "     ('clf', OneVsRestClassifier(RandomForestClassifier(max_depth=2, random_state=0)))])\n",
        "  \n",
        "  ## Train Classifier\n",
        "rfc_classifier.fit(X_train, Y_train)\n",
        "  \n",
        "  \n",
        "  ## Test Set Prediction Module\n",
        "testTweets = []\n",
        "testLabelGold = []\n",
        "csv.field_size_limit(500 * 1024 * 1024)  \n",
        "with open('/content/drive/MyDrive/Dataset/Final_Testing.txt', 'r') as f:\n",
        "  next(f) # skip headings\n",
        "  reader=csv.reader(f, dialect=\"excel-tab\")\n",
        "  for line in reader:\n",
        "    #print(line[2])\n",
        "    preProcessedTweetText= preProcessingModule(line[2])\n",
        "    #print(preProcessedTweetText)\n",
        "    testTweets.append(preProcessedTweetText)\n",
        "    if(line[1]==\"1\"):\n",
        "      testLabelGold.append(\"Fake\")\n",
        "    else:\n",
        "      testLabelGold.append(\"Not Fake\")\n",
        "  \n",
        "X_test = np.array(testTweets)\n",
        "testLabelPredicted = rfc_classifier.predict(X_test)\n",
        "#print(testLabelPredicted)\n",
        "\n",
        "# Evaluation\n",
        "results = confusion_matrix(testLabelGold, testLabelPredicted) \n",
        "    \n",
        "print ('Confusion Matrix :')\n",
        "print (results) \n",
        "\n",
        "print ('Recall Score :',recall_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Precision Score :',precision_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Macro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Weighted Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='weighted'))\n",
        "  \n",
        "print ('Accuracy :',accuracy_score(testLabelGold, testLabelPredicted)) \n",
        "  \n",
        "  \n",
        "print ('Evaluation Report : ')\n",
        "print (classification_report(testLabelGold, testLabelPredicted)) \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tyc2HoSTxK3h",
        "outputId": "53e144d4-f084-4e0a-bad0-f2f418ad0d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix :\n",
            "[[79 21]\n",
            " [42 58]]\n",
            "Recall Score : 0.685\n",
            "Precision Score : 0.6935348885866723\n",
            "Micro Avg. F1 Score : 0.685\n",
            "Macro Avg. F1 Score : 0.6814884097171314\n",
            "Micro Avg. F1 Score : 0.685\n",
            "Weighted Avg. F1 Score : 0.6814884097171313\n",
            "Accuracy : 0.685\n",
            "Evaluation Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.65      0.79      0.71       100\n",
            "    Not Fake       0.73      0.58      0.65       100\n",
            "\n",
            "    accuracy                           0.69       200\n",
            "   macro avg       0.69      0.69      0.68       200\n",
            "weighted avg       0.69      0.69      0.68       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "########For Multinomial Naive Bayes:########\n",
        "mnb_classifier = Pipeline([\n",
        "     ('count_vectorizer', CountVectorizer(ngram_range=(1, 3))),\n",
        "      ('tfidf', TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)),\n",
        "     ('clf', OneVsRestClassifier(MultinomialNB()))])\n",
        "  \n",
        "\n",
        "  \n",
        "  ## Train Classifier\n",
        "mnb_classifier.fit(X_train, Y_train)\n",
        "  \n",
        "  \n",
        "  ## Test Set Prediction Module\n",
        "testTweets = []\n",
        "testLabelGold = []\n",
        "csv.field_size_limit(500 * 1024 * 1024)  \n",
        "with open('/content/drive/MyDrive/Dataset/Final_Testing.txt', 'r') as f:\n",
        "  next(f) # skip headings\n",
        "  reader=csv.reader(f, dialect=\"excel-tab\")\n",
        "  for line in reader:\n",
        "    #print(line[2])\n",
        "    preProcessedTweetText= preProcessingModule(line[2])\n",
        "    #print(preProcessedTweetText)\n",
        "    testTweets.append(preProcessedTweetText)\n",
        "    if(line[1]==\"1\"):\n",
        "      testLabelGold.append(\"Fake\")\n",
        "    else:\n",
        "      testLabelGold.append(\"Not Fake\")\n",
        "  \n",
        "X_test = np.array(testTweets)\n",
        "testLabelPredicted = mnb_classifier.predict(X_test)\n",
        "#print(testLabelPredicted)\n",
        "\n",
        "# Evaluation\n",
        "results = confusion_matrix(testLabelGold, testLabelPredicted) \n",
        "    \n",
        "print ('Confusion Matrix :')\n",
        "print (results) \n",
        "\n",
        "print ('Recall Score :',recall_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Precision Score :',precision_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Macro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Weighted Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='weighted'))\n",
        "  \n",
        "print ('Accuracy :',accuracy_score(testLabelGold, testLabelPredicted)) \n",
        "\n",
        "print ('Evaluation Report : ')\n",
        "print (classification_report(testLabelGold, testLabelPredicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAvvyXvUxQRJ",
        "outputId": "047ec246-f1fe-488e-ca56-4a6b7d33dfd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix :\n",
            "[[81 19]\n",
            " [54 46]]\n",
            "Recall Score : 0.635\n",
            "Precision Score : 0.6538461538461539\n",
            "Micro Avg. F1 Score : 0.635\n",
            "Macro Avg. F1 Score : 0.6234687298517085\n",
            "Micro Avg. F1 Score : 0.635\n",
            "Weighted Avg. F1 Score : 0.6234687298517084\n",
            "Accuracy : 0.635\n",
            "Evaluation Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.60      0.81      0.69       100\n",
            "    Not Fake       0.71      0.46      0.56       100\n",
            "\n",
            "    accuracy                           0.64       200\n",
            "   macro avg       0.65      0.64      0.62       200\n",
            "weighted avg       0.65      0.64      0.62       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##for linear SVC\n",
        "lsvc_classifier = Pipeline([\n",
        "     ('count_vectorizer', CountVectorizer(ngram_range=(1, 3))),\n",
        "     ('tfidf', TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)),\n",
        "     ('clf', OneVsRestClassifier(LinearSVC(C=10.0, class_weight=None, dual=True, fit_intercept=True,\n",
        "      intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
        "      multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
        "      verbose=0)))])\n",
        "\n",
        "  ## Train Classifier\n",
        "lsvc_classifier.fit(X_train, Y_train)\n",
        "  \n",
        "  \n",
        "  ## Test Set Prediction Module\n",
        "testTweets = []\n",
        "testLabelGold = []\n",
        "csv.field_size_limit(500 * 1024 * 1024)  \n",
        "with open('/content/drive/MyDrive/Dataset/Final_Testing.txt', 'r') as f:\n",
        "    next(f) # skip headings\n",
        "    reader=csv.reader(f, dialect=\"excel-tab\")\n",
        "    for line in reader:\n",
        "          #print(line[2])\n",
        "        preProcessedTweetText= preProcessingModule(line[2])\n",
        "          #print(preProcessedTweetText)\n",
        "        testTweets.append(preProcessedTweetText)\n",
        "        if(line[1]==\"1\"):\n",
        "          testLabelGold.append(\"Fake\")\n",
        "        else:\n",
        "          testLabelGold.append(\"Not Fake\")\n",
        "  \n",
        "X_test = np.array(testTweets)\n",
        "testLabelPredicted = lsvc_classifier.predict(X_test)\n",
        "  #print(testLabelPredicted)\n",
        "\n",
        "  # Evaluation\n",
        "results = confusion_matrix(testLabelGold, testLabelPredicted) \n",
        "    \n",
        "print ('Confusion Matrix :')\n",
        "print (results) \n",
        "\n",
        "print ('Recall Score :',recall_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Precision Score :',precision_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Macro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Weighted Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='weighted'))\n",
        "  \n",
        "print ('Accuracy :',accuracy_score(testLabelGold, testLabelPredicted)) \n",
        "  \n",
        "  \n",
        "print ('Evaluation Report : ')\n",
        "print (classification_report(testLabelGold, testLabelPredicted)) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bX0yuuvxXCA",
        "outputId": "23d0986a-b742-4ba1-dde2-fedd02283c7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix :\n",
            "[[82 18]\n",
            " [47 53]]\n",
            "Recall Score : 0.675\n",
            "Precision Score : 0.6910688939840595\n",
            "Micro Avg. F1 Score : 0.675\n",
            "Macro Avg. F1 Score : 0.6680201230879235\n",
            "Micro Avg. F1 Score : 0.675\n",
            "Weighted Avg. F1 Score : 0.6680201230879236\n",
            "Accuracy : 0.675\n",
            "Evaluation Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.64      0.82      0.72       100\n",
            "    Not Fake       0.75      0.53      0.62       100\n",
            "\n",
            "    accuracy                           0.68       200\n",
            "   macro avg       0.69      0.68      0.67       200\n",
            "weighted avg       0.69      0.68      0.67       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "Cdn574OpmCMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########For Multilayer Perceptron Classifier:########\n",
        "mlp_classifier = Pipeline([\n",
        "     ('count_vectorizer', CountVectorizer(ngram_range=(1, 3))),\n",
        "     ('clf', OneVsRestClassifier(MLPClassifier(hidden_layer_sizes=(200,), random_state=1, max_iter=60, warm_start=True)))])\n",
        "  \n",
        "  ## Train Classifier\n",
        "mlp_classifier.fit(X_train, Y_train)\n",
        "  \n",
        "  \n",
        "  ## Test Set Prediction Module\n",
        "testTweets = []\n",
        "testLabelGold = []\n",
        "csv.field_size_limit(500 * 1024 * 1024)  \n",
        "with open('/content/drive/MyDrive/Dataset/Final_Testing.txt', 'r') as f:\n",
        "    next(f) # skip headings\n",
        "    reader=csv.reader(f, dialect=\"excel-tab\")\n",
        "    for line in reader:\n",
        "          #print(line[2])\n",
        "        preProcessedTweetText= preProcessingModule(line[2])\n",
        "          #print(preProcessedTweetText)\n",
        "        testTweets.append(preProcessedTweetText)\n",
        "        if(line[1]==\"1\"):\n",
        "          testLabelGold.append(\"Fake\")\n",
        "        else:\n",
        "          testLabelGold.append(\"Not Fake\")\n",
        "  \n",
        "X_test = np.array(testTweets)\n",
        "testLabelPredicted = lsvc_classifier.predict(X_test)\n",
        "  #print(testLabelPredicted)\n",
        "\n",
        "  # Evaluation\n",
        "results = confusion_matrix(testLabelGold, testLabelPredicted) \n",
        "    \n",
        "print ('Confusion Matrix :')\n",
        "print (results) \n",
        "\n",
        "print ('Recall Score :',recall_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Precision Score :',precision_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Macro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Weighted Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='weighted'))\n",
        "  \n",
        "print ('Accuracy :',accuracy_score(testLabelGold, testLabelPredicted)) \n",
        "  \n",
        "  \n",
        "print ('Evaluation Report : ')\n",
        "print (classification_report(testLabelGold, testLabelPredicted))"
      ],
      "metadata": {
        "id": "pR07fqJMcO0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "076ab992-7965-4d5d-a5c1-3b58fc425cf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix :\n",
            "[[82 18]\n",
            " [47 53]]\n",
            "Recall Score : 0.675\n",
            "Precision Score : 0.6910688939840595\n",
            "Micro Avg. F1 Score : 0.675\n",
            "Macro Avg. F1 Score : 0.6680201230879235\n",
            "Micro Avg. F1 Score : 0.675\n",
            "Weighted Avg. F1 Score : 0.6680201230879236\n",
            "Accuracy : 0.675\n",
            "Evaluation Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.64      0.82      0.72       100\n",
            "    Not Fake       0.75      0.53      0.62       100\n",
            "\n",
            "    accuracy                           0.68       200\n",
            "   macro avg       0.69      0.68      0.67       200\n",
            "weighted avg       0.69      0.68      0.67       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VElRTVy1kSSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##valid ensembling with five best classifier without second best MLP\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "vtc = VotingClassifier(estimators=[('rfc',rfc_classifier),('dt',dt_classifier) ,('sgd',sgd_classifier),('mnb',mnb_classifier),('lsvc',lsvc_classifier)], \n",
        "                       voting='hard',weights=[7,4,4,4,7])\n",
        "vtc.fit(X_train, Y_train)\n",
        "  \n",
        "  \n",
        "  ## Test Set Prediction Module\n",
        "testTweets = []\n",
        "testLabelGold = []\n",
        "csv.field_size_limit(500 * 1024 * 1024)  \n",
        "with open('/content/drive/MyDrive/Dataset/Final_Testing.txt', 'r') as f:\n",
        "    next(f) # skip headings\n",
        "    reader=csv.reader(f, dialect=\"excel-tab\")\n",
        "    for line in reader:\n",
        "          #print(line[2])\n",
        "        preProcessedTweetText= preProcessingModule(line[2])\n",
        "          #print(preProcessedTweetText)\n",
        "        testTweets.append(preProcessedTweetText)\n",
        "        if(line[1]==\"1\"):\n",
        "          testLabelGold.append(\"Fake\")\n",
        "        else:\n",
        "          testLabelGold.append(\"Not Fake\")\n",
        "  \n",
        "X_test = np.array(testTweets)\n",
        "testLabelPredicted = vtc.predict(X_test)\n",
        "  #print(testLabelPredicted)\n",
        "\n",
        "  # Evaluation\n",
        "results = confusion_matrix(testLabelGold, testLabelPredicted) \n",
        "    \n",
        "print ('Confusion Matrix :')\n",
        "print (results) \n",
        "\n",
        "print ('Recall Score :',recall_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Precision Score :',precision_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Macro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Weighted Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='weighted'))\n",
        "  \n",
        "print ('Accuracy :',accuracy_score(testLabelGold, testLabelPredicted)) \n",
        "  \n",
        "  \n",
        "print ('Evaluation Report : ')\n",
        "print (classification_report(testLabelGold, testLabelPredicted)) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piTzXUwcxa9J",
        "outputId": "6e03722f-df6d-4495-d483-9887ea874c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix :\n",
            "[[83 17]\n",
            " [46 54]]\n",
            "Recall Score : 0.685\n",
            "Precision Score : 0.7019871164974343\n",
            "Micro Avg. F1 Score : 0.685\n",
            "Macro Avg. F1 Score : 0.6782348885313723\n",
            "Micro Avg. F1 Score : 0.685\n",
            "Weighted Avg. F1 Score : 0.6782348885313723\n",
            "Accuracy : 0.685\n",
            "Evaluation Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.64      0.83      0.72       100\n",
            "    Not Fake       0.76      0.54      0.63       100\n",
            "\n",
            "    accuracy                           0.69       200\n",
            "   macro avg       0.70      0.69      0.68       200\n",
            "weighted avg       0.70      0.69      0.68       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##valid ensembling with five best classifier with second best MLP\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "vtc = VotingClassifier(estimators=[('rfc',rfc_classifier),('mlp',mlp_classifier) ,('sgd',sgd_classifier),('mnb',mnb_classifier),('lsvc',lsvc_classifier)], \n",
        "                       voting='hard',weights=[7,7,4,4,7])\n",
        "vtc.fit(X_train, Y_train)\n",
        "  \n",
        "  \n",
        "  ## Test Set Prediction Module\n",
        "testTweets = []\n",
        "testLabelGold = []\n",
        "csv.field_size_limit(500 * 1024 * 1024)  \n",
        "with open('/content/drive/MyDrive/Dataset/Final_Testing.txt', 'r') as f:\n",
        "    next(f) # skip headings\n",
        "    reader=csv.reader(f, dialect=\"excel-tab\")\n",
        "    for line in reader:\n",
        "          #print(line[2])\n",
        "        preProcessedTweetText= preProcessingModule(line[2])\n",
        "          #print(preProcessedTweetText)\n",
        "        testTweets.append(preProcessedTweetText)\n",
        "        if(line[1]==\"1\"):\n",
        "          testLabelGold.append(\"Fake\")\n",
        "        else:\n",
        "          testLabelGold.append(\"Not Fake\")\n",
        "  \n",
        "X_test = np.array(testTweets)\n",
        "testLabelPredicted = vtc.predict(X_test)\n",
        "  #print(testLabelPredicted)\n",
        "\n",
        "  # Evaluation\n",
        "results = confusion_matrix(testLabelGold, testLabelPredicted) \n",
        "    \n",
        "print ('Confusion Matrix :')\n",
        "print (results) \n",
        "\n",
        "print ('Recall Score :',recall_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Precision Score :',precision_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Macro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Weighted Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='weighted'))\n",
        "  \n",
        "print ('Accuracy :',accuracy_score(testLabelGold, testLabelPredicted)) \n",
        "  \n",
        "  \n",
        "print ('Evaluation Report : ')\n",
        "print (classification_report(testLabelGold, testLabelPredicted)) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MfBIUkSpQke",
        "outputId": "3dc01626-1b1f-4ada-b398-0553d135870c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix :\n",
            "[[87 13]\n",
            " [56 44]]\n",
            "Recall Score : 0.655\n",
            "Precision Score : 0.690160716476506\n",
            "Micro Avg. F1 Score : 0.655\n",
            "Macro Avg. F1 Score : 0.6382794684280884\n",
            "Micro Avg. F1 Score : 0.655\n",
            "Weighted Avg. F1 Score : 0.6382794684280885\n",
            "Accuracy : 0.655\n",
            "Evaluation Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.61      0.87      0.72       100\n",
            "    Not Fake       0.77      0.44      0.56       100\n",
            "\n",
            "    accuracy                           0.66       200\n",
            "   macro avg       0.69      0.66      0.64       200\n",
            "weighted avg       0.69      0.66      0.64       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Best three with MLP\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "vtc = VotingClassifier(estimators=[('rfc',rfc_classifier),('mlp',mlp_classifier),('lsvc',lsvc_classifier)], \n",
        "                       voting='hard',weights=[7,3,7])\n",
        "vtc.fit(X_train, Y_train)\n",
        "  \n",
        "  \n",
        "  ## Test Set Prediction Module\n",
        "testTweets = []\n",
        "testLabelGold = []\n",
        "csv.field_size_limit(500 * 1024 * 1024)  \n",
        "with open('/content/drive/MyDrive/Dataset/Final_Testing.txt', 'r') as f:\n",
        "    next(f) # skip headings\n",
        "    reader=csv.reader(f, dialect=\"excel-tab\")\n",
        "    for line in reader:\n",
        "          #print(line[2])\n",
        "        preProcessedTweetText= preProcessingModule(line[2])\n",
        "          #print(preProcessedTweetText)\n",
        "        testTweets.append(preProcessedTweetText)\n",
        "        if(line[1]==\"1\"):\n",
        "          testLabelGold.append(\"Fake\")\n",
        "        else:\n",
        "          testLabelGold.append(\"Not Fake\")\n",
        "  \n",
        "X_test = np.array(testTweets)\n",
        "testLabelPredicted = vtc.predict(X_test)\n",
        "  #print(testLabelPredicted)\n",
        "\n",
        "  # Evaluation\n",
        "results = confusion_matrix(testLabelGold, testLabelPredicted) \n",
        "    \n",
        "print ('Confusion Matrix :')\n",
        "print (results) \n",
        "\n",
        "print ('Recall Score :',recall_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Precision Score :',precision_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Macro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Weighted Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='weighted'))\n",
        "  \n",
        "print ('Accuracy :',accuracy_score(testLabelGold, testLabelPredicted)) \n",
        "  \n",
        "  \n",
        "print ('Evaluation Report : ')\n",
        "print (classification_report(testLabelGold, testLabelPredicted)) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "322rbNTHsKJz",
        "outputId": "ab36b947-b176-4630-9547-4debd7835645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix :\n",
            "[[87 13]\n",
            " [57 43]]\n",
            "Recall Score : 0.65\n",
            "Precision Score : 0.6860119047619048\n",
            "Micro Avg. F1 Score : 0.65\n",
            "Macro Avg. F1 Score : 0.6321984026902059\n",
            "Micro Avg. F1 Score : 0.65\n",
            "Weighted Avg. F1 Score : 0.6321984026902059\n",
            "Accuracy : 0.65\n",
            "Evaluation Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.60      0.87      0.71       100\n",
            "    Not Fake       0.77      0.43      0.55       100\n",
            "\n",
            "    accuracy                           0.65       200\n",
            "   macro avg       0.69      0.65      0.63       200\n",
            "weighted avg       0.69      0.65      0.63       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "vtc = VotingClassifier(estimators=[('rfc',rfc_classifier),('mnb',mnb_classifier),('lsvc',lsvc_classifier)], \n",
        "                       voting='hard',weights=[7,3,7])\n",
        "vtc.fit(X_train, Y_train)\n",
        "  \n",
        "  \n",
        "  ## Test Set Prediction Module\n",
        "testTweets = []\n",
        "testLabelGold = []\n",
        "csv.field_size_limit(500 * 1024 * 1024)  \n",
        "with open('/content/drive/MyDrive/Dataset/Final_Testing.txt', 'r') as f:\n",
        "    next(f) # skip headings\n",
        "    reader=csv.reader(f, dialect=\"excel-tab\")\n",
        "    for line in reader:\n",
        "          #print(line[2])\n",
        "        preProcessedTweetText= preProcessingModule(line[2])\n",
        "          #print(preProcessedTweetText)\n",
        "        testTweets.append(preProcessedTweetText)\n",
        "        if(line[1]==\"1\"):\n",
        "          testLabelGold.append(\"Fake\")\n",
        "        else:\n",
        "          testLabelGold.append(\"Not Fake\")\n",
        "  \n",
        "X_test = np.array(testTweets)\n",
        "testLabelPredicted = vtc.predict(X_test)\n",
        "  #print(testLabelPredicted)\n",
        "\n",
        "  # Evaluation\n",
        "results = confusion_matrix(testLabelGold, testLabelPredicted) \n",
        "    \n",
        "print ('Confusion Matrix :')\n",
        "print (results) \n",
        "\n",
        "print ('Recall Score :',recall_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Precision Score :',precision_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Macro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Weighted Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='weighted'))\n",
        "  \n",
        "print ('Accuracy :',accuracy_score(testLabelGold, testLabelPredicted)) \n",
        "  \n",
        "  \n",
        "print ('Evaluation Report : ')\n",
        "print (classification_report(testLabelGold, testLabelPredicted)) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2irEMmIxf3Z",
        "outputId": "9c9af16c-8991-474d-8241-1299b6292bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix :\n",
            "[[83 17]\n",
            " [46 54]]\n",
            "Recall Score : 0.685\n",
            "Precision Score : 0.7019871164974343\n",
            "Micro Avg. F1 Score : 0.685\n",
            "Macro Avg. F1 Score : 0.6782348885313723\n",
            "Micro Avg. F1 Score : 0.685\n",
            "Weighted Avg. F1 Score : 0.6782348885313723\n",
            "Accuracy : 0.685\n",
            "Evaluation Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.64      0.83      0.72       100\n",
            "    Not Fake       0.76      0.54      0.63       100\n",
            "\n",
            "    accuracy                           0.69       200\n",
            "   macro avg       0.70      0.69      0.68       200\n",
            "weighted avg       0.70      0.69      0.68       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "vtc = VotingClassifier(estimators=[('rfc',rfc_classifier),('sgd',sgd_classifier),('lsvc',lsvc_classifier)], \n",
        "                       voting='hard',weights=[5,3,5])\n",
        "vtc.fit(X_train, Y_train)\n",
        "  \n",
        "  \n",
        "  ## Test Set Prediction Module\n",
        "testTweets = []\n",
        "testLabelGold = []\n",
        "csv.field_size_limit(500 * 1024 * 1024)  \n",
        "with open('/content/drive/MyDrive/Dataset/Final_Testing.txt', 'r') as f:\n",
        "    next(f) # skip headings\n",
        "    reader=csv.reader(f, dialect=\"excel-tab\")\n",
        "    for line in reader:\n",
        "          #print(line[2])\n",
        "        preProcessedTweetText= preProcessingModule(line[2])\n",
        "          #print(preProcessedTweetText)\n",
        "        testTweets.append(preProcessedTweetText)\n",
        "        if(line[1]==\"1\"):\n",
        "          testLabelGold.append(\"Fake\")\n",
        "        else:\n",
        "          testLabelGold.append(\"Not Fake\")\n",
        "  \n",
        "X_test = np.array(testTweets)\n",
        "testLabelPredicted = vtc.predict(X_test)\n",
        "  #print(testLabelPredicted)\n",
        "\n",
        "  # Evaluation\n",
        "results = confusion_matrix(testLabelGold, testLabelPredicted) \n",
        "    \n",
        "print ('Confusion Matrix :')\n",
        "print (results) \n",
        "\n",
        "print ('Recall Score :',recall_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Precision Score :',precision_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Macro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='macro'))\n",
        "print ('Micro Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "print ('Weighted Avg. F1 Score :',f1_score(testLabelGold, testLabelPredicted, average='weighted'))\n",
        "  \n",
        "print ('Accuracy :',accuracy_score(testLabelGold, testLabelPredicted)) \n",
        "  \n",
        "  \n",
        "print ('Evaluation Report : ')\n",
        "print (classification_report(testLabelGold, testLabelPredicted)) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJb93inHypWq",
        "outputId": "736943ae-8b4d-436b-a39a-5a18ffe79a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix :\n",
            "[[83 17]\n",
            " [46 54]]\n",
            "Recall Score : 0.685\n",
            "Precision Score : 0.7019871164974343\n",
            "Micro Avg. F1 Score : 0.685\n",
            "Macro Avg. F1 Score : 0.6782348885313723\n",
            "Micro Avg. F1 Score : 0.685\n",
            "Weighted Avg. F1 Score : 0.6782348885313723\n",
            "Accuracy : 0.685\n",
            "Evaluation Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.64      0.83      0.72       100\n",
            "    Not Fake       0.76      0.54      0.63       100\n",
            "\n",
            "    accuracy                           0.69       200\n",
            "   macro avg       0.70      0.69      0.68       200\n",
            "weighted avg       0.70      0.69      0.68       200\n",
            "\n"
          ]
        }
      ]
    }
  ]
}